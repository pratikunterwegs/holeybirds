---
editor_options: 
  chunk_output_type: console
---

# Prepare tracking data by individual

## Load libraries

```{r}
# libs for data
library(data.table)
library(readxl)
library(stringi)
library(glue)

library(RSQLite)
```

### Get `atlastools`

```{r install_atlastools_2, eval=TRUE}
if (!require(remotes)) {
  install.packages("remotes", repos = "http://cran.us.r-project.org")
}

# installation using remotes
if (!require(atlastools)) {
  remotes::install_github("pratikunterwegs/atlastools")
}

library(atlastools)
```

## List data

```{r}
# list data files
data_files <- list.files(path = "data/raw", pattern = "sqlite")
names <- stri_replace_all(data_files, replacement = " ", regex = "\\_")

# split by word
names <- stri_extract_all_words(str = names)

# get genus name and treatment
species <- vapply(names, `[[`, FUN.VALUE = "string", 1)
# treatment <- vapply(names, `[[`, FUN.VALUE = "string", 3)

# data files again with full names
data_files <- list.files(path = "data/raw", pattern = "sqlite", full.names = TRUE)
```

## Load data and split by ID

Load and assign species and treatment.

```{r}
invisible(
  Map(data_files, species, f = function(file, sp) {
    # open connection
    con = dbConnect(
      drv = SQLite(),
      dbname = file
    )
    
    # list the tables
    table_name <- dbListTables(con)[3] # 3rd table is localisations
    
    # prepare to query all tables
    query <- sprintf('select * from \"%s\"', table_name)
    
    # query the database
    df <- dbGetQuery(conn = con, statement = query)
    
    # disconnect from database
    dbDisconnect(con)
    
    # make data table
    setDT(df)
    
    # assign species
    df[, sp := sp]

    # get unique tags
    tags <- unique(df$TAG)
    
    # remove *Z covariance
    df[, c("VARZ", "COVXZ", "COVYZ") := NULL]

    # split by tag
    df <- split(df, by = "TAG")

    # looping over tag ID, write to file
    invisible(
      Map(df, tags, f = function(dt, tag) {

        # write to file
        fwrite(
          x = dt,
          file = glue::glue(
            "data/processed/data_id/data_{sp}_{tag}.csv"
          )
        )
      })
    )
  })
)
```

## Get basic temporal tracking metrics

### Read files and get metrics

```{r}
# list data
data_files <- list.files("data/processed/data_id/", full.names = TRUE)

# get metrics
data_metrics <- lapply(data_files, function(file) {
  # read file
  df <- fread(file)

  # convert time
  df[, time := as.POSIXct(as.numeric(TIME) / 1000, tz = "UTC", origin = "1970-01-01")]
  # this is consistent with the
  # time column previously present

  df_summary <- df[, list(
    nfixes = .N,
    days_tracking = as.numeric(
      difftime(max(time), min(time),
        units = "days"
      )
    ),
    start_tracking = min(time),
    end_tracking = max(time)
  ),
  by = c("sp", "TAG")
  ]

  # temporal heatmap
  df[, day := as.Date(time)]
  df_tmp_heat <- df[, list(nfixes = .N),
    by = c("sp", "TAG", "day")
  ]
  rm(df)
  # return a list
  list(
    basic = df_summary,
    heatmap = df_tmp_heat
  )
})

# bind and get data
data_basic <- rbindlist(
  lapply(data_metrics, `[[`, "basic")
)

# write to file
fwrite(data_basic, "data/results/data_basic_tracking_metric.csv")
```

### Write daily fixes to file

```{r}
# get data from list
data_daily_heat <- rbindlist(
  lapply(data_metrics, `[[`, "heatmap")
)

# save data
fwrite(data_daily_heat, file = "data/results/data_daily_fixes.csv")
```

## Prepare RRV data for individuals

### Basic read-in

```{r read_rrv_files}
# list files
files <- list.files(path = "data/raw/", pattern = "xlsx", full.names = T)

# remove long term sparrows
files <- files[-grep("Long", files)]

# read data
data <- lapply(files, read_xlsx, skip = 1)

# there are warnings related to empty columns because excel is terrible
lapply(data, sapply, function(x) sum(is.na(x)))

# colnames
cols_to_keep <- c("Group", "ID", "Tag", "RRV", "Day", "Date", "calculated RRV")

# keep only useful rows and cols
data <- lapply(
  data, function(df) {
    setDT(df)

    # subset columns
    df <- df[, ..cols_to_keep]

    # remove extra rows by removing NAs
    na.omit(df)
  }
)
```

### Handle date and column names

```{r}
# first bind rows
data <- rbindlist(data)

# edit names to match previous individual data
setnames(data,
  old = c("Group", "Tag", "calculated RRV", "Date"),
  new = c("treat", "trunc_tag", "rrv_calc", "date")
)

# removes cols
data <- data[, !c("ID", "Day")]
```

### Link RRV to species and individual

```{r}
# read previously summarised data
data_tracking_summary <- fread("data/results/data_basic_tracking_metric.csv")

# select useful columns
# data_tracking_summary <- data_tracking_summary[, c("sp", "treat", "TAG")]

# get last four digits of the tag id
data_tracking_summary[, "tag_4_digit" := stri_sub(TAG, -4, -1)]

# pad the RRV data tag id, ie, make xx01 to 0001
data[, trunc_tag := stri_pad_left(trunc_tag, width = 4, pad = 0)]

data_treat = unique(data, by = c("trunc_tag", "treat"))

# attach treatment data to tracking metrics
data_tracking_summary_w_rrv =
  merge(
  data_tracking_summary, data_treat, all = F,
  by.y = c("trunc_tag"),
  by.x = c("tag_4_digit")
)

data_tracking_summary_w_rrv = data_tracking_summary_w_rrv[
  , !c("tag_4_digit", "date", "rrv_calc")
]

fwrite(data_tracking_summary_w_rrv, 
       file = "data/results/data_tracking_metrics_rrv.csv")

# join the datasets
data <- merge(data, data_tracking_summary,
  by.x = c("trunc_tag"),
  by.y = c("tag_4_digit"),
  all.x = T, all.y = F
)

# remove old tag and write to file
data <- data[, !"trunc_tag"]

fwrite(data, file = "data/results/data_daily_rrv.csv")
```

