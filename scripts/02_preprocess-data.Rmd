---
editor_options: 
  chunk_output_type: console
---

# Pre-processing tracking data

## Load libraries and prepare files

```{r}
# libs for data
library(data.table)
library(stringi)
library(glue)

# for analysis
library(recurse)

# library for atlas data
library(ggplot2)
library(atlastools)
```

Prepare files.

```{r}
# list files
files <- list.files("data/processed/data_id", full.names = TRUE)
```

## Pre-processing pipeline

This pre-processing pipeline is parameterised based on exploratory data analysis which can be found in the supplementary material.

```{r preproc_pipeline}
# remove old preprocessing log
if (file.exists("data/log_preprocessing.log")) {
  message("removing old preprocessing log")
  file.remove("data/log_preprocessing.log")
} else {
  message(glue("No preprocessing log as of {Sys.time()}, new log to be made"))
}

# some parameters
min_daily_fixes <- 500 # discard data below n rows
moving_window <- 5 # moving window
speed_threshold <- 3 # 3 m/s
angle_threshold <- 20 # in degrees
hour_start <- 5
hour_end <- 20
recurse_radius <- 10 # metres

# sink date time to log
sink(file = "data/log_preprocessing.log", append = FALSE)
glue("Holeybirds Pre-processing log from {Sys.time()}\n\n")
sink()

# open sinks
sink(file = "data/log_preprocessing.log", append = TRUE)

# run pre-processing
for (file in files) {
  # read in file
  df <- fread(file)

  tag_id <- unique(df$TAG_ID)
  species <- unique(df$sp)
  treatment <- unique(df$treat)

  # write messages
  # informative messages
  print(glue("Pre-processing tag_id = {tag_id}; {species}; {treatment}
                 N rows raw = {nrow(df)}
                 "))

  # filter on time of day -- this is being redone for safety
  # the column 'd' already marks daytime positions
  df[, c("date", "hour") := list(
    lubridate::date(as.POSIXct(UNIX, origin = "1970-01-01", tz = "Asia/Jerusalem")),
    lubridate::hour(as.POSIXct(UNIX, origin = "1970-01-01", tz = "Asia/Jerusalem"))
  )]

  df <- df[hour >= hour_start & hour < hour_end, ]

  print(
    glue("
          -- removed nighttime positions between {hour_end}PM and {hour_start}AM
             ")
  )

  # split by date
  df_l <- split(df, by = "date")

  # remove data with few rows
  df_l <- df_l[vapply(df_l, function(le) {
    nrow(le) > min_daily_fixes
  }, FUN.VALUE = T)]

  #### Filtering on speed ####
  # warn for all data lost, and skip to next individual if all data lost
  # else continue with pre-processing
  if (purrr::is_empty(df_l)) {
    print(
      glue::glue("List of data is empty, all data had fewer than \\
        {min_daily_fixes} rows")
    )

    print(
      glue("\n\n***\n\n")
    )

    next
  } else {
    # pre-processing continues here

    print(glue::glue("-- Pre-processing {length(df_l)} days' daytime data separately
                       "))

    # first calculate speeds and turning angles, handle 0 angles
    df_l <- lapply(df_l, function(le) {

      # check if filtering works
      le[, c("speed_in", "speed_out", "angle") := list(
        atl_get_speed(data = le, x = "x", time = "UNIX", type = "in"),
        atl_get_speed(data = le, time = "UNIX", type = "out"),
        atl_turning_angle(data = le, time = "UNIX")
      )]
      # fix missingle angles due to same position, assign 0
      le[, angle := nafill(angle, type = "const", fill = 0)]
    })

    # then filter on speed
    print(glue::glue("-- Filtered for speed < {speed_threshold} and angle < {angle_threshold}
                       "))

    df_l <- lapply(df_l, function(le) {
      atl_filter_covariates(
        le,
        filters = c(
          glue::glue("(speed_in < {speed_threshold} & speed_out < {speed_threshold}) | angle < {angle_threshold}
            ")
        )
      )
    })
  }

  #### Median smoothing ####
  # check again if any data remains and if not, move to next ID
  if (purrr::is_empty(df_l)) {
    print(
      glue::glue("List of data is empty, all data removed by speed filter
                  ")
    )

    print(
      glue("\n\n***\n\n")
    )

    next
  } else {

    # pre-processing continues
    # apply a median smooth smooth
    df_l <- lapply(df_l, atl_median_smooth,
      x = "x", y = "y",
      time = "UNIX", moving_window = moving_window
    )

    print(
      glue("-- Median smoothed coordinates, moving window = {moving_window}")
    )
  }

  #### Get Residence time ####
  # get residence times
  # print message when done
  print(
    glue::glue("-- Getting residence times, radius = {recurse_radius} m
                  ")
  )

  data_residence <- lapply(df_l, function(le) {

    # do basic recurse -- refer to Bracis et al. (2018) Ecography
    le_rt <- getRecursions(
      x = le[, c("x", "y", "UNIX", "TAG_ID")],
      radius = recurse_radius,
      timeunits = "mins"
    )$residenceTime

    # print message when done
    print(
      glue::glue("-- Res time for date {unique(le$date)} done
                   ")
    )

    le_rt
  })

  # add residence times
  df_l <- Map(function(pos_data, res_t_data) {
    setDT(pos_data)
    pos_data$res_time <- res_t_data

    # return the position data
    pos_data
  }, df_l, data_residence)

  print(
    glue::glue("-- Residence times assigned")
  )

  #### Merging data and saving to file ####
  df <- rbindlist(df_l)
  # save data to file
  save_file <- glue("data/processed/data_preprocessed/data_preproc\\
                      _{species}_{treatment}_{tag_id}.csv")
  print(
    glue("-- N rows pre-processed = {nrow(df)}")
  )

  # really save the data
  fwrite(df, file = save_file)

  print(
    glue("-- saved to data/processed/data_preprocessed/
                data_preproc_{species}_{treatment}_{tag_id}.csv
            ")
  )

  print(
    glue("\n\n***\n\n")
  )
}
sink()
```

