---
editor_options: 
  chunk_output_type: console
---

# Pre-processing tracking data

## Load libraries and prepare files

```{r}
# libs for data
library(data.table)
library(stringi)
library(glue)

# for analysis
library(dbscan)

# library for atlas data
library(ggplot2)
library(atlastools)
```

Prepare files.

```{r}
# list files
files <- list.files("data/processed/data_id", full.names = TRUE)
```

## Pre-processing pipeline

This pre-processing pipeline is parameterised based on exploratory data analysis which can be found in the supplementary material.

```{r preproc_pipeline}
# remove old preprocessing log
if (file.exists("data/log_preprocessing.log")) {
  message("removing old preprocessing log")
  file.remove("data/log_preprocessing.log")
} else {
  message(glue("No preprocessing log as of {Sys.time()}, new log to be made"))
}

# some parameters
min_daily_fixes <- 500 # discard data below n rows
moving_window_ppa <- 7 # moving window for sparrow, bulbul, warbler (PPA)
moving_window_h <- 3 # moving window for swallow
speed_threshold_ppa <- 3 # speed threshold for PPA is 3 m/s
speed_threshold_h <- 20 # speed threshold for swallow
angle_threshold <- 10 # in degrees
hour_start <- 5
hour_end <- 20
dbscan_eps  <- 3 # for PPA only

# sink date time to log
sink(file = "data/log_preprocessing.log", append = FALSE)
glue("Holeybirds Pre-processing log from {Sys.time()}\n\n")
sink()

# open sinks
sink(file = "data/log_preprocessing.log", append = TRUE)

# run pre-processing
for (file in files) {
  # read in file
  df <- fread(file)
  
  tag_id <- unique(df$TAG_ID)
  species <- unique(df$sp)
  treatment <- unique(df$treat)
  
  # write messages
  # informative messages
  print(glue("Pre-processing tag_id = {tag_id}; {species}; {treatment}
                 N rows raw = {nrow(df)}
                 "))
  
  # filter on time of day -- this is being redone for safety
  # the column 'd' already marks daytime positions
  df[, c("date", "hour") := list(
    lubridate::date(as.POSIXct(UNIX, origin = "1970-01-01", tz = "Asia/Jerusalem")),
    lubridate::hour(as.POSIXct(UNIX, origin = "1970-01-01", tz = "Asia/Jerusalem"))
  )]
  
  df <- df[hour >= hour_start & hour < hour_end, ]
  
  print(
    glue("
          -- removed nighttime positions between {hour_end}PM and {hour_start}AM
             ")
  )
  
  # split by date
  df_l <- split(df, by = "date")
  
  # remove data with few rows
  df_l <- df_l[vapply(df_l, function(le) {
    nrow(le) > min_daily_fixes
  }, FUN.VALUE = T)]
  
  #### Filtering on speed OR density based scan (DBSCAN) ####
  # warn for all data lost, and skip to next individual if all data lost
  # else continue with pre-processing
  if (purrr::is_empty(df_l)) {
    print(
      glue::glue("List of data is empty, all data had fewer than \\
        {min_daily_fixes} rows")
    )
    
    print(
      glue("\n\n***\n\n")
    )
    
    next
  } else {
    # pre-processing continues here
    
    print(glue::glue("-- Pre-processing {length(df_l)} days' data separately
                       "))
    
    # first calculate speeds and turning angles, handle 0 angles
    df_l <- lapply(df_l, function(le) {
      
      # check if filtering works
      le[, c("speed_in", "speed_out", "angle") := list(
        atl_get_speed(data = le, x = "x", time = "UNIX", type = "in"),
        atl_get_speed(data = le, time = "UNIX", type = "out"),
        atl_turning_angle(data = le, time = "UNIX")
      )]
      # fix missing angles due to same position, assign 0
      le[, angle := nafill(angle, type = "const", fill = 0)]
      
      # classify dbscan outlier
      scan = dbscan(le[, c("x", "y")], eps = 3)
      le[, outlier:= scan$cluster == 0]
    })
    
    # check is species is Hirundo (swallow) and handle differently
    if(species == "Hirundo") {
      df_l <- lapply(df_l, function(le) {
        atl_filter_covariates(
          le,
          filters = c(
            glue::glue("(speed_in < {speed_threshold_h} & speed_out\\
            < {speed_threshold_h}) | angle < {angle_threshold}
                       ")
          )
        )
      })
      print(glue::glue("-- Filtered to remove unreal speeds; speeds\\
      < {speed_threshold_h} OR angle < {angle_threshold}
                       "))
    } else {
      df_l <- lapply(df_l, function(le) {
        atl_filter_covariates(
          le,
          filters = c(
            glue::glue("((!outlier)) | ((angle < {angle_threshold}) &\\
                       (angle != 0) & (speed_in > {speed_threshold_ppa}) &\\
                       (speed_out > {speed_threshold_ppa}))")
          )
        )
      })
      print(glue::glue("-- Filtered to keep transit; speeds > \\
      {speed_threshold_ppa} AND angle < {angle_threshold};
          ELSE removing DBSCAN outliers (eps = {dbscan_eps})
                       "))
    }
    
    
    #### Median smoothing ####
    # remove data with few rows
    df_l <- df_l[vapply(df_l, function(le) {
      nrow(le) > min_daily_fixes
    }, FUN.VALUE = T)]
    
    # check again if any data remains and if not, move to next ID
    if (purrr::is_empty(df_l)) {
      print(
        glue::glue("List of data is empty, all data removed by speed filter
                  ")
      )
      
      print(
        glue("\n\n***\n\n")
      )
      
      next
    } else {
      
      if(species == "Hirundo") {
        # pre-processing continues
        # apply a median smooth smooth
        df_l <- lapply(df_l, atl_median_smooth,
                       x = "x", y = "y",
                       time = "UNIX", moving_window = moving_window_h
        )
        
        print(
          glue("-- Median smoothed coordinates, moving window = {moving_window_h}")
        )  
      } else {
        # apply a median smooth smooth
        df_l <- lapply(df_l, atl_median_smooth,
                       x = "x", y = "y",
                       time = "UNIX", moving_window = moving_window_ppa
        )
        
        print(
          glue("-- Median smoothed coordinates, moving window = {moving_window_ppa}")
        )
      } # if else for H or PPA ends
      
      #### Merging data and saving to file ####
      df <- rbindlist(df_l)
      # save data to file
      save_file <- glue("data/processed/data_preprocessed/data_preproc\\
                          _{species}_{treatment}_{tag_id}.csv")
      print(
        glue("-- N rows pre-processed = {nrow(df)}")
      )
      
      # really save the data
      fwrite(df, file = save_file)
      
      print(
        glue("-- saved to data/processed/data_preprocessed/
                    data_preproc_{species}_{treatment}_{tag_id}.csv
                ")
      )
      
      print(
        glue("\n\n***\n\n")
      )
    } # else case for data after filtering ends, data to file
  } # else case for min daily fixes data ends
} # for loops ends
sink()
```
  
  
